{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ffeda847a3345ba8adee1f014e3729d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f2a69ac841144b7bad9d8ece63eb7cf",
              "IPY_MODEL_3f164cc9041c43cb9add10320ce3d1f4",
              "IPY_MODEL_68a4f3c07f554d6184d050f526cabd4c"
            ],
            "layout": "IPY_MODEL_b534155ff83346fbb7381979a5119f73"
          }
        },
        "3f2a69ac841144b7bad9d8ece63eb7cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc74f448701d46afa13658fbeb10739a",
            "placeholder": "​",
            "style": "IPY_MODEL_71e120a89148484eb271a89eb9c4b694",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3f164cc9041c43cb9add10320ce3d1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e77550dc9ed4b4ab411f56ed2a5b596",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdfec9e185384446a11ae530458eeea5",
            "value": 25
          }
        },
        "68a4f3c07f554d6184d050f526cabd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69a6aa794de14779bab26b425d98714a",
            "placeholder": "​",
            "style": "IPY_MODEL_7010f1baf0c94803bc01811ef38674b6",
            "value": " 25.0/25.0 [00:00&lt;00:00, 426B/s]"
          }
        },
        "b534155ff83346fbb7381979a5119f73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc74f448701d46afa13658fbeb10739a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71e120a89148484eb271a89eb9c4b694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e77550dc9ed4b4ab411f56ed2a5b596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdfec9e185384446a11ae530458eeea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69a6aa794de14779bab26b425d98714a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7010f1baf0c94803bc01811ef38674b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28f52b5bfd734437998eb2ba40f40549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_970ef066959f4e4192840c579fb1b447",
              "IPY_MODEL_8db0e79c85874e239611d616007fcaa2",
              "IPY_MODEL_0cb7683583d7420b81cc3ff25d3be859"
            ],
            "layout": "IPY_MODEL_a62ad88adcc04aa297006dca3229517a"
          }
        },
        "970ef066959f4e4192840c579fb1b447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d5c96ddc82f45348eb4e1c2b91c02b0",
            "placeholder": "​",
            "style": "IPY_MODEL_ee00ea506a10479e8227990986670b1a",
            "value": "config.json: 100%"
          }
        },
        "8db0e79c85874e239611d616007fcaa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9467ff3749ab44228f855f6dcf50da6e",
            "max": 1230,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_160b344189fd4ea18655b19ab4856cd4",
            "value": 1230
          }
        },
        "0cb7683583d7420b81cc3ff25d3be859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27c0710d44d446919832f4e5e8be7e8b",
            "placeholder": "​",
            "style": "IPY_MODEL_7b22e5adccf8491c966ca44f685c31d0",
            "value": " 1.23k/1.23k [00:00&lt;00:00, 19.1kB/s]"
          }
        },
        "a62ad88adcc04aa297006dca3229517a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d5c96ddc82f45348eb4e1c2b91c02b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee00ea506a10479e8227990986670b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9467ff3749ab44228f855f6dcf50da6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "160b344189fd4ea18655b19ab4856cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27c0710d44d446919832f4e5e8be7e8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b22e5adccf8491c966ca44f685c31d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "483b93f610824569a3b0c71bac0634cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d06a8dbf3584828a601615214bd07a9",
              "IPY_MODEL_201d42a1618247f0a5d8825670eb74cb",
              "IPY_MODEL_67bc645e7342473cae3ab3ddedbb6225"
            ],
            "layout": "IPY_MODEL_7909f1649d514dc8abdc681812b5da4e"
          }
        },
        "0d06a8dbf3584828a601615214bd07a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb59a8010a7747d8aca49bb1fe6fc46c",
            "placeholder": "​",
            "style": "IPY_MODEL_c673290701df48edbc823eaabeca53f7",
            "value": "spiece.model: 100%"
          }
        },
        "201d42a1618247f0a5d8825670eb74cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b20b94690d3444fb0d41b43d1eea5aa",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c475e121200f44a3adba3afc6da15857",
            "value": 791656
          }
        },
        "67bc645e7342473cae3ab3ddedbb6225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec79d79b2ee643879bd8a7dcbf5272c5",
            "placeholder": "​",
            "style": "IPY_MODEL_afdf788311fe4694ba91c4a84f029972",
            "value": " 792k/792k [00:00&lt;00:00, 3.12MB/s]"
          }
        },
        "7909f1649d514dc8abdc681812b5da4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb59a8010a7747d8aca49bb1fe6fc46c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c673290701df48edbc823eaabeca53f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b20b94690d3444fb0d41b43d1eea5aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c475e121200f44a3adba3afc6da15857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec79d79b2ee643879bd8a7dcbf5272c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afdf788311fe4694ba91c4a84f029972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12b18a27479b40d4aac173da7a7c6340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ed4ea9d5cce40998c7a6266f8b13724",
              "IPY_MODEL_93fb8f56a32043869f09c5d48c6fd271",
              "IPY_MODEL_0be28b5b61ba49718e103e345171def6"
            ],
            "layout": "IPY_MODEL_f232ce5ce5804135942b64932bd62521"
          }
        },
        "2ed4ea9d5cce40998c7a6266f8b13724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b407cd288d040fa964aee7d23950e59",
            "placeholder": "​",
            "style": "IPY_MODEL_7bea53132ccc450db63f34b0e8b5eab7",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "93fb8f56a32043869f09c5d48c6fd271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d562c84ec68a46f896b5fe939625cc89",
            "max": 1786,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0284d02af924333a0496c07a25f7be2",
            "value": 1786
          }
        },
        "0be28b5b61ba49718e103e345171def6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2de0ca79d22047878e461248791cfdaa",
            "placeholder": "​",
            "style": "IPY_MODEL_c317ae0505ae43a0bd0f3b14d3edcd2e",
            "value": " 1.79k/1.79k [00:00&lt;00:00, 31.6kB/s]"
          }
        },
        "f232ce5ce5804135942b64932bd62521": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b407cd288d040fa964aee7d23950e59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bea53132ccc450db63f34b0e8b5eab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d562c84ec68a46f896b5fe939625cc89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0284d02af924333a0496c07a25f7be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2de0ca79d22047878e461248791cfdaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c317ae0505ae43a0bd0f3b14d3edcd2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ee0686517b84f2eace2b70b054bf9c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07b6d6137bc7463f9101cf84ab309fec",
              "IPY_MODEL_bc231852f15e473ab84bce460cc31ed9",
              "IPY_MODEL_e76656c1f10d4715b7ba7451a78c8863"
            ],
            "layout": "IPY_MODEL_ac3fdefda1df4f8cadf28241184c6d38"
          }
        },
        "07b6d6137bc7463f9101cf84ab309fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06497c34aa75439786d4dee5dafa0f73",
            "placeholder": "​",
            "style": "IPY_MODEL_a2c9524eddbc453482ccd2683f67e256",
            "value": "model.safetensors: 100%"
          }
        },
        "bc231852f15e473ab84bce460cc31ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9401912ca11493fa4bd7afdc8dc169e",
            "max": 1187738336,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81b96286093c485395ef605dec2d09e6",
            "value": 1187738336
          }
        },
        "e76656c1f10d4715b7ba7451a78c8863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1abea6be122944afa4293a8f143e2df8",
            "placeholder": "​",
            "style": "IPY_MODEL_a8c5b67915f949b299ba2552879af65d",
            "value": " 1.19G/1.19G [00:26&lt;00:00, 33.8MB/s]"
          }
        },
        "ac3fdefda1df4f8cadf28241184c6d38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06497c34aa75439786d4dee5dafa0f73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2c9524eddbc453482ccd2683f67e256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9401912ca11493fa4bd7afdc8dc169e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81b96286093c485395ef605dec2d09e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1abea6be122944afa4293a8f143e2df8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8c5b67915f949b299ba2552879af65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#TESTING:\n",
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install accelerate\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoModelForSequenceClassification, AutoModelForQuestionAnswering, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from rouge_score import rouge_scorer\n",
        "import json\n",
        "import math\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "print(\"JAI MATA DI!\")\n",
        "input_file_path = 'train-v2.0.json'\n",
        "with open(input_file_path, 'r') as f:\n",
        "  squad_data = json.load(f)\n",
        "\n",
        "squad_examples = squad_data['data']\n",
        "\n",
        "rouge_metric = load_metric(\"rouge\")\n",
        "\n",
        "qa_model_name = \"deepset/roberta-base-squad2\"\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "\n",
        "nli_model_name = \"facebook/bart-large-mnli\"\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name)\n",
        "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
        "\n",
        "clm_model_name = \"gpt2\"\n",
        "clm_model = AutoModelForCausalLM.from_pretrained(clm_model_name)\n",
        "clm_tokenizer = AutoTokenizer.from_pretrained(clm_model_name)\n",
        "\n",
        "\n",
        "qa_model.to(device)\n",
        "nli_model.to(device)\n",
        "clm_model.to(device)\n",
        "\n",
        "def generate_answers(question, context, top_n=5):\n",
        "    inputs = qa_tokenizer(question, context, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = qa_model(**inputs)\n",
        "    start_probs = F.softmax(outputs.start_logits, dim=1).squeeze(0)\n",
        "    end_probs = F.softmax(outputs.end_logits, dim=1).squeeze(0)\n",
        "    max_answer_length = 30\n",
        "    answer_spans = []\n",
        "    for start_idx, start_prob in enumerate(start_probs):\n",
        "        for end_idx, end_prob in enumerate(end_probs[start_idx:start_idx + max_answer_length]):\n",
        "            score = start_prob * end_prob\n",
        "            answer_spans.append((start_idx, start_idx + end_idx, score))\n",
        "    answer_spans = sorted(answer_spans, key=lambda x: x[2], reverse=True)[:top_n]\n",
        "    answers = []\n",
        "    for start_idx, end_idx, score in answer_spans:\n",
        "        answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
        "        answer = qa_tokenizer.decode(answer_tokens)\n",
        "        answers.append((answer, score.item()))\n",
        "    return answers\n",
        "\n",
        "\n",
        "# CHECKING THE SEMANTIC SIMILARITY USING THE BIDIRECTIONAL ENTAILMENT ALGORITHM.RESEARCH PAPER:https://arxiv.org/pdf/1911.00681.pdf\n",
        "# Bi-directional entailment involves checking whether two texts (typically a hypothesis and a premise) can entail each other, implying a deep semantic similarity or paraphrase relationship.\n",
        "# Mathematical Basis of Bi-directional Entailment:\n",
        "# To implement bi-directional entailment, each text is considered both as a hypothesis and a premise against the other text. This involves two checks:\n",
        "# Forward Entailment: Whether the premise (first text) semantically entails the hypothesis (second text).\n",
        "# Backward Entailment: Whether the hypothesis (second text) semantically entails the premise (first text).\n",
        "# If both conditions are met, the texts are considered semantically equivalent, akin to paraphrases. Mathematically, this is often represented using probabilities derived from a model trained on entailment tasks, such as those derived from the MNLI dataset using a BERT model.\n",
        "\n",
        "# Given probabilities of entailment (P) from a softmax layer for both forward and backward directions, the odds of entailment are calculated as:\n",
        "# Odds = P/1-P ​\n",
        "# The final score for bi-directional entailment could be the product of the odds for both directions, ensuring that high entailment probabilities in both directions yield a higher score\n",
        "def check_entailment(premise, hypothesis):\n",
        "    inputs = nli_tokenizer(premise, hypothesis, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = nli_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    entail_contradiction_logits = logits[:, [0, 2]]\n",
        "    probs = F.softmax(entail_contradiction_logits, dim=1)\n",
        "    entail_prob = probs[:, 0].item()\n",
        "    return entail_prob\n",
        "\n",
        "def cluster_answers(answers, question):\n",
        "    answers = [answer[0] for answer in answers]\n",
        "    clusters = []\n",
        "    for answer in answers:\n",
        "        added_to_cluster = False\n",
        "        for cluster in clusters:\n",
        "            representative_answer = cluster[0]\n",
        "            forward_entail_prob = check_entailment(question + \" \" + representative_answer, answer)\n",
        "            backward_entail_prob = check_entailment(question + \" \" + answer, representative_answer)\n",
        "            if forward_entail_prob > 0.4 and backward_entail_prob > 0.4:\n",
        "                cluster.append(answer)\n",
        "                added_to_cluster = True\n",
        "                break\n",
        "        if not added_to_cluster:\n",
        "            clusters.append([answer])\n",
        "    return clusters\n",
        "\n",
        "def calculate_average_clusters(all_question_data):\n",
        "    total_correct_clusters = 0\n",
        "    total_incorrect_clusters = 0\n",
        "    num_correct_questions = 0\n",
        "    num_incorrect_questions = 0\n",
        "    for question_data in all_question_data:\n",
        "        num_clusters = len(question_data[\"clusters\"])\n",
        "        is_correct = any(answer[\"correct\"] for answer in question_data[\"generated_answers\"])\n",
        "        if is_correct:\n",
        "            total_correct_clusters += num_clusters\n",
        "            num_correct_questions += 1\n",
        "        else:\n",
        "            total_incorrect_clusters += num_clusters\n",
        "            num_incorrect_questions += 1\n",
        "    average_correct_clusters = total_correct_clusters / num_correct_questions if num_correct_questions > 0 else 0\n",
        "    average_incorrect_clusters = total_incorrect_clusters / num_incorrect_questions if num_incorrect_questions > 0 else 0\n",
        "    return average_correct_clusters, average_incorrect_clusters\n",
        "\n",
        "def calculate_ptrue(question, generated_answers, top_n=5):\n",
        "    # Format the prompt\n",
        "    prompt = f\"Question: {question}\\nHere are some brainstormed ideas:\\n\"\n",
        "    for answer, _ in generated_answers[:top_n]:\n",
        "        prompt += answer + \"\\n\"\n",
        "    prompt += \"Possible Answer: {}\\nIs the possible answer: (A) True (B) False\\nThe possible answer is:\"\n",
        "\n",
        "    inputs = qa_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    # print(qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = qa_model(**inputs)\n",
        "        start_logits = outputs.start_logits[0]\n",
        "        end_logits = outputs.end_logits[0]\n",
        "\n",
        "        # Find the token index of \"True\" in the prompt\n",
        "        true_token_id = qa_tokenizer.convert_tokens_to_ids(\"ĠTrue\")\n",
        "        true_token_indices = (inputs[\"input_ids\"][0] == true_token_id).nonzero(as_tuple=True)[0]\n",
        "        if true_token_indices.nelement() == 0:\n",
        "          print(f\"Warning: 'True' token not found in prompt for question: {question}\")\n",
        "          return 0.5\n",
        "        else:\n",
        "          true_token_index = true_token_indices[0].item()\n",
        "        # Calculate the score for each answer span based on start/end logits\n",
        "        answer_span_scores = []\n",
        "        for start_idx, start_logit in enumerate(start_logits):\n",
        "            for end_idx, end_logit in enumerate(end_logits[start_idx:]):\n",
        "                real_end_idx = start_idx + end_idx\n",
        "                if start_idx <= true_token_index <= real_end_idx:  # Check if span includes \"True\"\n",
        "                    score = start_logit + end_logit\n",
        "                    answer_span_scores.append(score)\n",
        "                else:\n",
        "                    answer_span_scores.append(torch.tensor(-float(\"inf\")).to(device))  # Assign very low score\n",
        "\n",
        "        # Find the answer span with the highest score\n",
        "        scores_tensor = torch.tensor(answer_span_scores)\n",
        "        scores_softmax = F.softmax(scores_tensor, dim=0)\n",
        "        p_true = scores_softmax[torch.argmax(scores_tensor)].item()\n",
        "\n",
        "    return p_true\n",
        "def calculate_lexical_similarity(answers, reference_answers):\n",
        "    if not reference_answers:\n",
        "        return 0.5\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "    scores = []\n",
        "    for answer in answers:\n",
        "        max_score = max(scorer.score(answer, ref)['rouge1'].fmeasure for ref in reference_answers)\n",
        "        scores.append(max_score)\n",
        "    average_score = sum(scores) / len(scores) if scores else 0.5\n",
        "    return average_score\n",
        "\n",
        "def calculate_semantic_entropy(cluster_probabilities):\n",
        "    entropy = 0\n",
        "    for prob in cluster_probabilities:\n",
        "        if prob > 0:\n",
        "            entropy -= prob * math.log2(prob)\n",
        "    return entropy\n",
        "\n",
        "def calculate_seq_log_prob(question, context, answer, model, tokenizer):\n",
        "    input_text = question + ' ' + context\n",
        "    output_text = answer\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    output_ids = tokenizer.encode(output_text, return_tensors='pt').to(device)\n",
        "\n",
        "    log_prob_sum = 0\n",
        "\n",
        "    for i in range(1, len(output_ids[0])):\n",
        "        previous_tokens = output_ids[:, :i]\n",
        "        target_token = output_ids[:, i]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(previous_tokens)\n",
        "            logits = outputs.logits[:, i-1, :]\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            log_prob = log_probs[0, target_token].item()\n",
        "\n",
        "        log_prob_sum += log_prob\n",
        "\n",
        "    seq_log_prob = log_prob_sum / len(output_ids[0])\n",
        "    return seq_log_prob\n",
        "\n",
        "all_question_data = []\n",
        "correct_output = []\n",
        "max_iterations = 1\n",
        "all_context_data=[]\n",
        "auroc_ptrue =0\n",
        "auroc_lexical =0\n",
        "auroc_entropy =0.15\n",
        "x=0\n",
        "\n",
        "for context_idx, squad_example in enumerate(squad_examples):\n",
        "    print(\"iteration number: \",x)\n",
        "    if x>= max_iterations:\n",
        "        break\n",
        "    x+=1\n",
        "    context_data = {\"context_index\": context_idx, \"questions\": []}\n",
        "    print(\"PARA ITERATION\")\n",
        "    k = 0\n",
        "    for paragraph in squad_example['paragraphs']:\n",
        "        k += 1\n",
        "        if k % 10 == 0:\n",
        "            print(k)\n",
        "        context = paragraph['context']\n",
        "        for qa in paragraph['qas']:\n",
        "            question = qa['question']\n",
        "            reference_answers = [answer['text'] for answer in qa['answers']]\n",
        "            question_data = {\"question\": question, \"reference_answers\": reference_answers, \"generated_answers\": []}\n",
        "\n",
        "            # Generate answers using the model (on GPU)\n",
        "            model_answers_with_confidence = generate_answers(question, context, top_n=5)\n",
        "\n",
        "            # Evaluate and store correct/incorrect labels\n",
        "            for answer, confidence in model_answers_with_confidence:\n",
        "                if not reference_answers:\n",
        "                    # print(f\"Skipping question with no reference answers: {question}\")\n",
        "                    continue\n",
        "                rouge_score = rouge_metric.compute(predictions=[answer], references=reference_answers, rouge_types=[\"rougeL\"])\n",
        "                rouge_l_score = rouge_score[\"rougeL\"].mid.fmeasure\n",
        "                correct_output.append(1 if rouge_l_score > 0.3 else 0)\n",
        "                question_data[\"generated_answers\"].append({\"answer\": answer, \"confidence\": confidence, \"rougeL_score\": rouge_l_score, \"correct\": correct_output[-1]})\n",
        "\n",
        "            # Calculate p(True) and store it in question_data\n",
        "\n",
        "            # Calculate lexical similarity and store it in question_data\n",
        "            if reference_answers:\n",
        "                question_data[\"lexical_similarity\"] = calculate_lexical_similarity([answer[0] for answer in model_answers_with_confidence], reference_answers)\n",
        "                question_data[\"p_true\"] = calculate_ptrue(question, model_answers_with_confidence, top_n=5)\n",
        "                clusters = cluster_answers(model_answers_with_confidence, question)\n",
        "                question_data[\"clusters\"] = clusters\n",
        "                context_data[\"questions\"].append(question_data)\n",
        "\n",
        "    all_context_data.append(context_data)\n",
        "\n",
        "# Calculate average clusters for all questions\n",
        "average_correct_clusters, average_incorrect_clusters = calculate_average_clusters([q for c in all_context_data for q in c[\"questions\"]])\n",
        "\n",
        "# Add average cluster information to each context\n",
        "for context_data in all_context_data:\n",
        "    context_data[\"average_correct_clusters\"] = average_correct_clusters\n",
        "    context_data[\"average_incorrect_clusters\"] = average_incorrect_clusters\n",
        "\n",
        "# Store data in a JSON file\n",
        "with open(\"context_question_clustering_data.json\", \"w\") as f:\n",
        "    json.dump(all_context_data, f, indent=4)\n",
        "\n",
        "# Iterate over the data and calculate sequence log probabilities\n",
        "with open(\"context_question_clustering_data.json\", \"r\") as f:\n",
        "    all_context_data = json.load(f)\n",
        "for context_data in all_context_data:\n",
        "    context_index = context_data[\"context_index\"]\n",
        "    context = squad_examples[context_index][\"paragraphs\"][0][\"context\"]\n",
        "    for question_data in context_data[\"questions\"]:\n",
        "        question = question_data[\"question\"]\n",
        "        reference_answers = question_data[\"reference_answers\"]\n",
        "        generated_answers = question_data[\"generated_answers\"]\n",
        "        log_probs = []\n",
        "        for answer_dict in generated_answers:\n",
        "            answer = answer_dict[\"answer\"]\n",
        "            log_prob = calculate_seq_log_prob(question, context, answer, clm_model, clm_tokenizer)\n",
        "            log_probs.append(log_prob)\n",
        "            answer_dict[\"log_prob\"] = log_prob\n",
        "        avg_log_prob = sum(log_probs) / len(log_probs)\n",
        "        question_data[\"avg_log_prob\"] = avg_log_prob\n",
        "with open(\"context_question_clustering_data.json\", \"w\") as f:\n",
        "    json.dump(all_context_data, f, indent=4)\n",
        "\n",
        "all_question_entropy_data=[]\n",
        "# Calculate semantic entropy for each question and store in a new JSON fileall_question_entropy_data = []\n",
        "with open(\"context_question_clustering_data.json\", \"r\") as f:\n",
        "    all_context_data = json.load(f)\n",
        "for context_data in all_context_data:\n",
        "    for question_data in context_data[\"questions\"]:\n",
        "        clusters = question_data[\"clusters\"]\n",
        "        answer_confidences = {answer[\"answer\"]: answer[\"confidence\"] for answer in question_data[\"generated_answers\"]}\n",
        "        total_prob=sum(sum(answer_confidences[answer] for answer in cluster) for cluster in clusters)\n",
        "        cluster_probabilities = [\n",
        "            sum(answer_confidences[answer] for answer in cluster)/total_prob for cluster in clusters\n",
        "        ]\n",
        "        semantic_entropy = calculate_semantic_entropy(cluster_probabilities)\n",
        "        question_data[\"semantic_entropy\"] = semantic_entropy\n",
        "        all_question_entropy_data.append(question_data)\n",
        "with open(\"question_entropy_data.json\", \"w\") as f:\n",
        "    json.dump(all_question_entropy_data, f, indent=4)\n",
        "\n",
        "ptrue_values = []\n",
        "lexical_similarity_values = []\n",
        "entropy_values = []\n",
        "correct_labels = []\n",
        "for question_data in all_question_entropy_data:\n",
        "    for answer in question_data[\"generated_answers\"]:\n",
        "        ptrue_values.append(question_data[\"p_true\"])\n",
        "        lexical_similarity_values.append(question_data[\"lexical_similarity\"])\n",
        "        entropy_values.append(question_data[\"semantic_entropy\"])\n",
        "        correct_labels.append(answer[\"correct\"])\n",
        "\n",
        "# Calculate AUROC scores\n",
        "auroc_ptrue += roc_auc_score(correct_labels, ptrue_values)\n",
        "auroc_lexical += roc_auc_score(correct_labels, lexical_similarity_values)\n",
        "auroc_entropy += roc_auc_score(correct_labels, entropy_values)\n",
        "\n",
        "# Store AUROC scores in a dictionary\n",
        "auroc_scores = {\n",
        "    \"p_true\": auroc_ptrue,\n",
        "    \"lexical_similarity\": auroc_lexical,\n",
        "    \"semantic_entropy\": auroc_entropy\n",
        "}\n",
        "\n",
        "# Save AUROC scores to a JSON file\n",
        "with open(\"auroc_scores.json\", \"w\") as f:\n",
        "    json.dump(auroc_scores, f, indent=4)\n",
        "\n",
        "# Print AUROC scores\n",
        "print(\"AUROC Scores:\")\n",
        "for metric, score in auroc_scores.items():\n",
        "    print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Unob1LCfdpm",
        "outputId": "a0322fb0-a56e-42ec-e979-b7e16dd10ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "JAI MATA DI!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration number:  0\n",
            "PARA ITERATION\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_question_entropy_data=[]\n",
        "# Calculate semantic entropy for each question and store in a new JSON fileall_question_entropy_data = []\n",
        "with open(\"context_question_clustering_data.json\", \"r\") as f:\n",
        "    all_context_data = json.load(f)\n",
        "for context_data in all_context_data:\n",
        "    for question_data in context_data[\"questions\"]:\n",
        "        clusters = question_data[\"clusters\"]\n",
        "        answer_confidences = {answer[\"answer\"]: answer[\"confidence\"] for answer in question_data[\"generated_answers\"]}\n",
        "        total_prob=sum(sum(answer_confidences[answer] for answer in cluster) for cluster in clusters)\n",
        "        cluster_probabilities = [\n",
        "            sum(answer_confidences[answer] for answer in cluster)/total_prob for cluster in clusters\n",
        "        ]\n",
        "        semantic_entropy = calculate_semantic_entropy(cluster_probabilities)\n",
        "        question_data[\"semantic_entropy\"] = semantic_entropy\n",
        "        all_question_entropy_data.append(question_data)\n",
        "with open(\"question_entropy_data.json\", \"w\") as f:\n",
        "    json.dump(all_question_entropy_data, f, indent=4)\n",
        "\n",
        "ptrue_values = []\n",
        "lexical_similarity_values = []\n",
        "entropy_values = []\n",
        "correct_labels = []\n",
        "for question_data in all_question_entropy_data:\n",
        "    for answer in question_data[\"generated_answers\"]:\n",
        "        ptrue_values.append(question_data[\"p_true\"])\n",
        "        lexical_similarity_values.append(question_data[\"lexical_similarity\"])\n",
        "        entropy_values.append(question_data[\"semantic_entropy\"])\n",
        "        correct_labels.append(answer[\"correct\"])\n",
        "\n",
        "# Calculate AUROC scores\n",
        "auroc_ptrue = roc_auc_score(correct_labels, ptrue_values)\n",
        "auroc_lexical = roc_auc_score(correct_labels, lexical_similarity_values)\n",
        "auroc_entropy = roc_auc_score(correct_labels, entropy_values)\n",
        "\n",
        "# Store AUROC scores in a dictionary\n",
        "auroc_scores = {\n",
        "    \"p_true\": auroc_ptrue,\n",
        "    \"lexical_similarity\": auroc_lexical,\n",
        "    \"semantic_entropy\": auroc_entropy\n",
        "}\n",
        "\n",
        "# Save AUROC scores to a JSON file\n",
        "with open(\"auroc_scores.json\", \"w\") as f:\n",
        "    json.dump(auroc_scores, f, indent=4)\n",
        "\n",
        "# Print AUROC scores\n",
        "print(\"AUROC Scores:\")\n",
        "for metric, score in auroc_scores.items():\n",
        "    print(f\"{metric}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM-W1dXLqfAn",
        "outputId": "8b296c1c-571d-4a29-cd55-5c2a179f503b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC Scores:\n",
            "p_true: 0.5042\n",
            "lexical_similarity: 0.8203\n",
            "semantic_entropy: 0.6023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTING:\n",
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install accelerate\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoModelForSequenceClassification, AutoModelForQuestionAnswering, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from rouge_score import rouge_scorer\n",
        "import json\n",
        "import math\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "print(\"JAI MATA DI!\")\n",
        "input_file_path = 'train-v2.0.json'\n",
        "with open(input_file_path, 'r') as f:\n",
        "  squad_data = json.load(f)\n",
        "\n",
        "squad_examples = squad_data['data']\n",
        "\n",
        "rouge_metric = load_metric(\"rouge\")\n",
        "\n",
        "qa_model_name = \"deepset/roberta-base-squad2\"\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "\n",
        "nli_model_name = \"facebook/bart-large-mnli\"\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name)\n",
        "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
        "\n",
        "clm_model_name = \"gpt2\"\n",
        "clm_model = AutoModelForCausalLM.from_pretrained(clm_model_name)\n",
        "clm_tokenizer = AutoTokenizer.from_pretrained(clm_model_name)\n",
        "\n",
        "\n",
        "qa_model.to(device)\n",
        "nli_model.to(device)\n",
        "clm_model.to(device)\n",
        "\n",
        "def generate_answers(question, context, top_n=5):\n",
        "    inputs = qa_tokenizer(question, context, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = qa_model(**inputs)\n",
        "    start_probs = F.softmax(outputs.start_logits, dim=1).squeeze(0)\n",
        "    end_probs = F.softmax(outputs.end_logits, dim=1).squeeze(0)\n",
        "    max_answer_length = 30\n",
        "    answer_spans = []\n",
        "    for start_idx, start_prob in enumerate(start_probs):\n",
        "        for end_idx, end_prob in enumerate(end_probs[start_idx:start_idx + max_answer_length]):\n",
        "            score = start_prob * end_prob\n",
        "            answer_spans.append((start_idx, start_idx + end_idx, score))\n",
        "    answer_spans = sorted(answer_spans, key=lambda x: x[2], reverse=True)[:top_n]\n",
        "    answers = []\n",
        "    for start_idx, end_idx, score in answer_spans:\n",
        "        answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
        "        answer = qa_tokenizer.decode(answer_tokens)\n",
        "        answers.append((answer, score.item()))\n",
        "    return answers\n",
        "\n",
        "\n",
        "# CHECKING THE SEMANTIC SIMILARITY USING THE BIDIRECTIONAL ENTAILMENT ALGORITHM.RESEARCH PAPER:https://arxiv.org/pdf/1911.00681.pdf\n",
        "# Bi-directional entailment involves checking whether two texts (typically a hypothesis and a premise) can entail each other, implying a deep semantic similarity or paraphrase relationship.\n",
        "# Mathematical Basis of Bi-directional Entailment:\n",
        "# To implement bi-directional entailment, each text is considered both as a hypothesis and a premise against the other text. This involves two checks:\n",
        "# Forward Entailment: Whether the premise (first text) semantically entails the hypothesis (second text).\n",
        "# Backward Entailment: Whether the hypothesis (second text) semantically entails the premise (first text).\n",
        "# If both conditions are met, the texts are considered semantically equivalent, akin to paraphrases. Mathematically, this is often represented using probabilities derived from a model trained on entailment tasks, such as those derived from the MNLI dataset using a BERT model.\n",
        "\n",
        "# Given probabilities of entailment (P) from a softmax layer for both forward and backward directions, the odds of entailment are calculated as:\n",
        "# Odds = P/1-P ​\n",
        "# The final score for bi-directional entailment could be the product of the odds for both directions, ensuring that high entailment probabilities in both directions yield a higher score\n",
        "def check_entailment(premise, hypothesis):\n",
        "    inputs = nli_tokenizer(premise, hypothesis, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = nli_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    entail_contradiction_logits = logits[:, [0, 2]]\n",
        "    probs = F.softmax(entail_contradiction_logits, dim=1)\n",
        "    entail_prob = probs[:, 0].item()\n",
        "    return entail_prob\n",
        "\n",
        "def cluster_answers(answers, question):\n",
        "    answers = [answer[0] for answer in answers]\n",
        "    clusters = []\n",
        "    for answer in answers:\n",
        "        added_to_cluster = False\n",
        "        for cluster in clusters:\n",
        "            representative_answer = cluster[0]\n",
        "            forward_entail_prob = check_entailment(question + \" \" + representative_answer, answer)\n",
        "            backward_entail_prob = check_entailment(question + \" \" + answer, representative_answer)\n",
        "            if forward_entail_prob > 0.2 and backward_entail_prob > 0.2:\n",
        "                cluster.append(answer)\n",
        "                added_to_cluster = True\n",
        "                break\n",
        "        if not added_to_cluster:\n",
        "            clusters.append([answer])\n",
        "    return clusters\n",
        "\n",
        "def calculate_average_clusters(all_question_data):\n",
        "    total_correct_clusters = 0\n",
        "    total_incorrect_clusters = 0\n",
        "    num_correct_questions = 0\n",
        "    num_incorrect_questions = 0\n",
        "    for question_data in all_question_data:\n",
        "        num_clusters = len(question_data[\"clusters\"])\n",
        "        is_correct = any(answer[\"correct\"] for answer in question_data[\"generated_answers\"])\n",
        "        if is_correct:\n",
        "            total_correct_clusters += num_clusters\n",
        "            num_correct_questions += 1\n",
        "        else:\n",
        "            total_incorrect_clusters += num_clusters\n",
        "            num_incorrect_questions += 1\n",
        "    average_correct_clusters = total_correct_clusters / num_correct_questions if num_correct_questions > 0 else 0\n",
        "    average_incorrect_clusters = total_incorrect_clusters / num_incorrect_questions if num_incorrect_questions > 0 else 0\n",
        "    return average_correct_clusters, average_incorrect_clusters\n",
        "\n",
        "def calculate_ptrue(question, generated_answers, top_n=5):\n",
        "    # Format the prompt\n",
        "    prompt = f\"Question: {question}\\nHere are some brainstormed ideas:\\n\"\n",
        "    for answer, _ in generated_answers[:top_n]:\n",
        "        prompt += answer + \"\\n\"\n",
        "    prompt += \"Possible Answer: {}\\nIs the possible answer: (A) True (B) False\\nThe possible answer is:\"\n",
        "\n",
        "    inputs = qa_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    # print(qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = qa_model(**inputs)\n",
        "        start_logits = outputs.start_logits[0]\n",
        "        end_logits = outputs.end_logits[0]\n",
        "\n",
        "        # Find the token index of \"True\" in the prompt\n",
        "        true_token_id = qa_tokenizer.convert_tokens_to_ids(\"ĠTrue\")\n",
        "        true_token_indices = (inputs[\"input_ids\"][0] == true_token_id).nonzero(as_tuple=True)[0]\n",
        "        if true_token_indices.nelement() == 0:\n",
        "          print(f\"Warning: 'True' token not found in prompt for question: {question}\")\n",
        "          return 0.5\n",
        "        else:\n",
        "          true_token_index = true_token_indices[0].item()\n",
        "        # Calculate the score for each answer span based on start/end logits\n",
        "        answer_span_scores = []\n",
        "        for start_idx, start_logit in enumerate(start_logits):\n",
        "            for end_idx, end_logit in enumerate(end_logits[start_idx:]):\n",
        "                real_end_idx = start_idx + end_idx\n",
        "                if start_idx <= true_token_index <= real_end_idx:  # Check if span includes \"True\"\n",
        "                    score = start_logit + end_logit\n",
        "                    answer_span_scores.append(score)\n",
        "                else:\n",
        "                    answer_span_scores.append(torch.tensor(-float(\"inf\")).to(device))  # Assign very low score\n",
        "\n",
        "        # Find the answer span with the highest score\n",
        "        scores_tensor = torch.tensor(answer_span_scores)\n",
        "        scores_softmax = F.softmax(scores_tensor, dim=0)\n",
        "        p_true = scores_softmax[torch.argmax(scores_tensor)].item()\n",
        "\n",
        "    return p_true\n",
        "def calculate_lexical_similarity(answers, reference_answers):\n",
        "    if not reference_answers:\n",
        "        return 0.5\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "    scores = []\n",
        "    for answer in answers:\n",
        "        max_score = max(scorer.score(answer, ref)['rouge1'].fmeasure for ref in reference_answers)\n",
        "        scores.append(max_score)\n",
        "    average_score = sum(scores) / len(scores) if scores else 0.5\n",
        "    return average_score\n",
        "\n",
        "def calculate_semantic_entropy(cluster_probabilities):\n",
        "    entropy = 0\n",
        "    for prob in cluster_probabilities:\n",
        "        if prob > 0:\n",
        "            entropy -= prob * math.log2(prob)\n",
        "    return entropy\n",
        "\n",
        "def calculate_seq_log_prob(question, context, answer, model, tokenizer):\n",
        "    input_text = question + ' ' + context\n",
        "    output_text = answer\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    output_ids = tokenizer.encode(output_text, return_tensors='pt').to(device)\n",
        "\n",
        "    log_prob_sum = 0\n",
        "\n",
        "    for i in range(1, len(output_ids[0])):\n",
        "        previous_tokens = output_ids[:, :i]\n",
        "        target_token = output_ids[:, i]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(previous_tokens)\n",
        "            logits = outputs.logits[:, i-1, :]\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            log_prob = log_probs[0, target_token].item()\n",
        "\n",
        "        log_prob_sum += log_prob\n",
        "\n",
        "    seq_log_prob = log_prob_sum / len(output_ids[0])\n",
        "    return seq_log_prob\n",
        "\n",
        "all_question_data = []\n",
        "correct_output = []\n",
        "max_iterations = 1\n",
        "all_context_data=[]\n",
        "x=0\n",
        "\n",
        "for context_idx, squad_example in enumerate(squad_examples):\n",
        "    print(\"iteration number: \",x)\n",
        "    if x>= max_iterations:\n",
        "        break\n",
        "    x+=1\n",
        "    context_data = {\"context_index\": context_idx, \"questions\": []}\n",
        "    print(\"PARA ITERATION\")\n",
        "    k = 0\n",
        "    for paragraph in squad_example['paragraphs']:\n",
        "        k += 1\n",
        "        if k % 10 == 0:\n",
        "            print(k)\n",
        "        context = paragraph['context']\n",
        "        for qa in paragraph['qas']:\n",
        "            question = qa['question']\n",
        "            reference_answers = [answer['text'] for answer in qa['answers']]\n",
        "            question_data = {\"question\": question, \"reference_answers\": reference_answers, \"generated_answers\": []}\n",
        "\n",
        "            # Generate answers using the model (on GPU)\n",
        "            model_answers_with_confidence = generate_answers(question, context, top_n=5)\n",
        "\n",
        "            # Evaluate and store correct/incorrect labels\n",
        "            for answer, confidence in model_answers_with_confidence:\n",
        "                if not reference_answers:\n",
        "                    # print(f\"Skipping question with no reference answers: {question}\")\n",
        "                    continue\n",
        "                rouge_score = rouge_metric.compute(predictions=[answer], references=reference_answers, rouge_types=[\"rougeL\"])\n",
        "                rouge_l_score = rouge_score[\"rougeL\"].mid.fmeasure\n",
        "                correct_output.append(1 if rouge_l_score > 0.3 else 0)\n",
        "                question_data[\"generated_answers\"].append({\"answer\": answer, \"confidence\": confidence, \"rougeL_score\": rouge_l_score, \"correct\": correct_output[-1]})\n",
        "\n",
        "            # Calculate p(True) and store it in question_data\n",
        "\n",
        "            # Calculate lexical similarity and store it in question_data\n",
        "            if reference_answers:\n",
        "                question_data[\"lexical_similarity\"] = calculate_lexical_similarity([answer[0] for answer in model_answers_with_confidence], reference_answers)\n",
        "                question_data[\"p_true\"] = calculate_ptrue(question, model_answers_with_confidence, top_n=5)\n",
        "                clusters = cluster_answers(model_answers_with_confidence, question)\n",
        "                question_data[\"clusters\"] = clusters\n",
        "                context_data[\"questions\"].append(question_data)\n",
        "\n",
        "    all_context_data.append(context_data)\n",
        "\n",
        "# Calculate average clusters for all questions\n",
        "average_correct_clusters, average_incorrect_clusters = calculate_average_clusters([q for c in all_context_data for q in c[\"questions\"]])\n",
        "\n",
        "# Add average cluster information to each context\n",
        "for context_data in all_context_data:\n",
        "    context_data[\"average_correct_clusters\"] = average_correct_clusters\n",
        "    context_data[\"average_incorrect_clusters\"] = average_incorrect_clusters\n",
        "\n",
        "# Store data in a JSON file\n",
        "with open(\"context_question_clustering_data.json\", \"w\") as f:\n",
        "    json.dump(all_context_data, f, indent=4)\n",
        "\n",
        "# Iterate over the data and calculate sequence log probabilities\n",
        "with open(\"context_question_clustering_data.json\", \"r\") as f:\n",
        "    all_context_data = json.load(f)\n",
        "for context_data in all_context_data:\n",
        "    context_index = context_data[\"context_index\"]\n",
        "    context = squad_examples[context_index][\"paragraphs\"][0][\"context\"]\n",
        "    for question_data in context_data[\"questions\"]:\n",
        "        question = question_data[\"question\"]\n",
        "        reference_answers = question_data[\"reference_answers\"]\n",
        "        generated_answers = question_data[\"generated_answers\"]\n",
        "        log_probs = []\n",
        "        for answer_dict in generated_answers:\n",
        "            answer = answer_dict[\"answer\"]\n",
        "            log_prob = calculate_seq_log_prob(question, context, answer, clm_model, clm_tokenizer)\n",
        "            log_probs.append(log_prob)\n",
        "            answer_dict[\"log_prob\"] = log_prob\n",
        "        avg_log_prob = sum(log_probs) / len(log_probs)\n",
        "        question_data[\"avg_log_prob\"] = avg_log_prob\n",
        "with open(\"context_question_clustering_data.json\", \"w\") as f:\n",
        "    json.dump(all_context_data, f, indent=4)\n",
        "\n",
        "all_question_entropy_data=[]\n",
        "# Calculate semantic entropy for each question and store in a new JSON fileall_question_entropy_data = []\n",
        "with open(\"context_question_clustering_data.json\", \"r\") as f:\n",
        "    all_context_data = json.load(f)\n",
        "for context_data in all_context_data:\n",
        "    for question_data in context_data[\"questions\"]:\n",
        "        clusters = question_data[\"clusters\"]\n",
        "        answer_confidences = {answer[\"answer\"]: answer[\"confidence\"] for answer in question_data[\"generated_answers\"]}\n",
        "        total_prob=sum(sum(answer_confidences[answer] for answer in cluster) for cluster in clusters)\n",
        "        cluster_probabilities = [\n",
        "            sum(answer_confidences[answer] for answer in cluster/total_prob) for cluster in clusters\n",
        "        ]\n",
        "        semantic_entropy = calculate_semantic_entropy(cluster_probabilities)\n",
        "        question_data[\"semantic_entropy\"] = semantic_entropy\n",
        "        all_question_entropy_data.append(question_data)\n",
        "with open(\"question_entropy_data.json\", \"w\") as f:\n",
        "    json.dump(all_question_entropy_data, f, indent=4)\n",
        "\n",
        "ptrue_values = []\n",
        "lexical_similarity_values = []\n",
        "entropy_values = []\n",
        "correct_labels = []\n",
        "for question_data in all_question_entropy_data:\n",
        "    for answer in question_data[\"generated_answers\"]:\n",
        "        ptrue_values.append(question_data[\"p_true\"])\n",
        "        lexical_similarity_values.append(question_data[\"lexical_similarity\"])\n",
        "        entropy_values.append(question_data[\"semantic_entropy\"])\n",
        "        correct_labels.append(answer[\"correct\"])\n",
        "\n",
        "# Calculate AUROC scores\n",
        "auroc_ptrue = roc_auc_score(correct_labels, ptrue_values)\n",
        "auroc_lexical = roc_auc_score(correct_labels, lexical_similarity_values)\n",
        "auroc_entropy = roc_auc_score(correct_labels, entropy_values)\n",
        "\n",
        "# Store AUROC scores in a dictionary\n",
        "auroc_scores = {\n",
        "    \"p_true\": auroc_ptrue,\n",
        "    \"lexical_similarity\": auroc_lexical,\n",
        "    \"semantic_entropy\": auroc_entropy\n",
        "}\n",
        "\n",
        "# Save AUROC scores to a JSON file\n",
        "with open(\"auroc_scores.json\", \"w\") as f:\n",
        "    json.dump(auroc_scores, f, indent=4)\n",
        "\n",
        "# Print AUROC scores\n",
        "print(\"AUROC Scores:\")\n",
        "for metric, score in auroc_scores.items():\n",
        "    print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D6lQwh_Og0q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT MAIN CODE\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "model_name = \"mrm8488/t5-base-finetuned-squadv2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# Get the number of parameters\n",
        "num_params = model.num_parameters()\n",
        "\n",
        "print(f\"Number of parameters in {model_name}: {num_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342,
          "referenced_widgets": [
            "8ffeda847a3345ba8adee1f014e3729d",
            "3f2a69ac841144b7bad9d8ece63eb7cf",
            "3f164cc9041c43cb9add10320ce3d1f4",
            "68a4f3c07f554d6184d050f526cabd4c",
            "b534155ff83346fbb7381979a5119f73",
            "bc74f448701d46afa13658fbeb10739a",
            "71e120a89148484eb271a89eb9c4b694",
            "4e77550dc9ed4b4ab411f56ed2a5b596",
            "cdfec9e185384446a11ae530458eeea5",
            "69a6aa794de14779bab26b425d98714a",
            "7010f1baf0c94803bc01811ef38674b6",
            "28f52b5bfd734437998eb2ba40f40549",
            "970ef066959f4e4192840c579fb1b447",
            "8db0e79c85874e239611d616007fcaa2",
            "0cb7683583d7420b81cc3ff25d3be859",
            "a62ad88adcc04aa297006dca3229517a",
            "1d5c96ddc82f45348eb4e1c2b91c02b0",
            "ee00ea506a10479e8227990986670b1a",
            "9467ff3749ab44228f855f6dcf50da6e",
            "160b344189fd4ea18655b19ab4856cd4",
            "27c0710d44d446919832f4e5e8be7e8b",
            "7b22e5adccf8491c966ca44f685c31d0",
            "483b93f610824569a3b0c71bac0634cb",
            "0d06a8dbf3584828a601615214bd07a9",
            "201d42a1618247f0a5d8825670eb74cb",
            "67bc645e7342473cae3ab3ddedbb6225",
            "7909f1649d514dc8abdc681812b5da4e",
            "eb59a8010a7747d8aca49bb1fe6fc46c",
            "c673290701df48edbc823eaabeca53f7",
            "5b20b94690d3444fb0d41b43d1eea5aa",
            "c475e121200f44a3adba3afc6da15857",
            "ec79d79b2ee643879bd8a7dcbf5272c5",
            "afdf788311fe4694ba91c4a84f029972",
            "12b18a27479b40d4aac173da7a7c6340",
            "2ed4ea9d5cce40998c7a6266f8b13724",
            "93fb8f56a32043869f09c5d48c6fd271",
            "0be28b5b61ba49718e103e345171def6",
            "f232ce5ce5804135942b64932bd62521",
            "6b407cd288d040fa964aee7d23950e59",
            "7bea53132ccc450db63f34b0e8b5eab7",
            "d562c84ec68a46f896b5fe939625cc89",
            "b0284d02af924333a0496c07a25f7be2",
            "2de0ca79d22047878e461248791cfdaa",
            "c317ae0505ae43a0bd0f3b14d3edcd2e",
            "0ee0686517b84f2eace2b70b054bf9c5",
            "07b6d6137bc7463f9101cf84ab309fec",
            "bc231852f15e473ab84bce460cc31ed9",
            "e76656c1f10d4715b7ba7451a78c8863",
            "ac3fdefda1df4f8cadf28241184c6d38",
            "06497c34aa75439786d4dee5dafa0f73",
            "a2c9524eddbc453482ccd2683f67e256",
            "d9401912ca11493fa4bd7afdc8dc169e",
            "81b96286093c485395ef605dec2d09e6",
            "1abea6be122944afa4293a8f143e2df8",
            "a8c5b67915f949b299ba2552879af65d"
          ]
        },
        "id": "DATzRtIM4uOX",
        "outputId": "a7aca229-35fd-490b-a8e9-7638ce1db888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ffeda847a3345ba8adee1f014e3729d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28f52b5bfd734437998eb2ba40f40549"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "483b93f610824569a3b0c71bac0634cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12b18a27479b40d4aac173da7a7c6340"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ee0686517b84f2eace2b70b054bf9c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at mrm8488/t5-base-finetuned-squadv2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in mrm8488/t5-base-finetuned-squadv2: 222905090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhCJ17mU1Dii"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install accelerate\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoModelForSequenceClassification, AutoModelForQuestionAnswering, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from rouge_score import rouge_scorer\n",
        "import json\n",
        "import math\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import accelerate\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = 'train-v2.0.json'\n",
        "with open(input_file_path, 'r') as f:\n",
        "  squad_data = json.load(f)\n",
        "\n",
        "squad_examples = squad_data['data']\n",
        "\n",
        "rouge_metric = load_metric(\"rouge\")\n",
        "\n",
        "qa_model_name = \"deepset/roberta-base-squad2\"\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "\n",
        "nli_model_name = \"facebook/bart-large-mnli\"\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name)\n",
        "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
        "\n",
        "clm_model_name = \"gpt2\"\n",
        "clm_model = AutoModelForCausalLM.from_pretrained(clm_model_name)\n",
        "clm_tokenizer = AutoTokenizer.from_pretrained(clm_model_name)\n",
        "\n",
        "\n",
        "qa_model.to(device)\n",
        "nli_model.to(device)\n",
        "clm_model.to(device)\n"
      ],
      "metadata": {
        "id": "e-bjc7q4Wc6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answers(question, context, top_n=5):\n",
        "    inputs = qa_tokenizer(question, context, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = qa_model(**inputs)\n",
        "    start_probs = F.softmax(outputs.start_logits, dim=1).squeeze(0)\n",
        "    end_probs = F.softmax(outputs.end_logits, dim=1).squeeze(0)\n",
        "    max_answer_length = 30\n",
        "    answer_spans = []\n",
        "    for start_idx, start_prob in enumerate(start_probs):\n",
        "        for end_idx, end_prob in enumerate(end_probs[start_idx:start_idx + max_answer_length]):\n",
        "            score = start_prob * end_prob\n",
        "            answer_spans.append((start_idx, start_idx + end_idx, score))\n",
        "    answer_spans = sorted(answer_spans, key=lambda x: x[2], reverse=True)[:top_n]\n",
        "    answers = []\n",
        "    for start_idx, end_idx, score in answer_spans:\n",
        "        answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n",
        "        answer = qa_tokenizer.decode(answer_tokens)\n",
        "        answers.append((answer, score.item()))\n",
        "    return answers\n",
        "\n",
        "\n",
        "# CHECKING THE SEMANTIC SIMILARITY USING THE BIDIRECTIONAL ENTAILMENT ALGORITHM.RESEARCH PAPER:https://arxiv.org/pdf/1911.00681.pdf\n",
        "# Bi-directional entailment involves checking whether two texts (typically a hypothesis and a premise) can entail each other, implying a deep semantic similarity or paraphrase relationship.\n",
        "# Mathematical Basis of Bi-directional Entailment:\n",
        "# To implement bi-directional entailment, each text is considered both as a hypothesis and a premise against the other text. This involves two checks:\n",
        "# Forward Entailment: Whether the premise (first text) semantically entails the hypothesis (second text).\n",
        "# Backward Entailment: Whether the hypothesis (second text) semantically entails the premise (first text).\n",
        "# If both conditions are met, the texts are considered semantically equivalent, akin to paraphrases. Mathematically, this is often represented using probabilities derived from a model trained on entailment tasks, such as those derived from the MNLI dataset using a BERT model.\n",
        "\n",
        "# Given probabilities of entailment (P) from a softmax layer for both forward and backward directions, the odds of entailment are calculated as:\n",
        "# Odds = P/1-P ​\n",
        "# The final score for bi-directional entailment could be the product of the odds for both directions, ensuring that high entailment probabilities in both directions yield a higher score\n",
        "def check_entailment(premise, hypothesis):\n",
        "    inputs = nli_tokenizer(premise, hypothesis, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = nli_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    entail_contradiction_logits = logits[:, [0, 2]]\n",
        "    probs = F.softmax(entail_contradiction_logits, dim=1)\n",
        "    entail_prob = probs[:, 0].item()\n",
        "    return entail_prob\n",
        "\n",
        "def cluster_answers(answers, question):\n",
        "    answers = [answer[0] for answer in answers]\n",
        "    clusters = []\n",
        "    for answer in answers:\n",
        "        added_to_cluster = False\n",
        "        for cluster in clusters:\n",
        "            representative_answer = cluster[0]\n",
        "            forward_entail_prob = check_entailment(question + \" \" + representative_answer, answer)\n",
        "            backward_entail_prob = check_entailment(question + \" \" + answer, representative_answer)\n",
        "            if forward_entail_prob > 0.4 and backward_entail_prob > 0.4:\n",
        "                cluster.append(answer)\n",
        "                added_to_cluster = True\n",
        "                break\n",
        "        if not added_to_cluster:\n",
        "            clusters.append([answer])\n",
        "    return clusters\n",
        "\n",
        "def calculate_average_clusters(all_question_data):\n",
        "    total_correct_clusters = 0\n",
        "    total_incorrect_clusters = 0\n",
        "    num_correct_questions = 0\n",
        "    num_incorrect_questions = 0\n",
        "    for question_data in all_question_data:\n",
        "        num_clusters = len(question_data[\"clusters\"])\n",
        "        is_correct = any(answer[\"correct\"] for answer in question_data[\"generated_answers\"])\n",
        "        if is_correct:\n",
        "            total_correct_clusters += num_clusters\n",
        "            num_correct_questions += 1\n",
        "        else:\n",
        "            total_incorrect_clusters += num_clusters\n",
        "            num_incorrect_questions += 1\n",
        "    average_correct_clusters = total_correct_clusters / num_correct_questions if num_correct_questions > 0 else 0\n",
        "    average_incorrect_clusters = total_incorrect_clusters / num_incorrect_questions if num_incorrect_questions > 0 else 0\n",
        "    return average_correct_clusters, average_incorrect_clusters"
      ],
      "metadata": {
        "id": "TB8QxkouyRGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ptrue(question, generated_answers, top_n=5):\n",
        "    # Format the prompt\n",
        "    prompt = f\"Question: {question}\\nHere are some brainstormed ideas:\\n\"\n",
        "    for answer, _ in generated_answers[:top_n]:\n",
        "        prompt += answer + \"\\n\"\n",
        "    prompt += \"Possible Answer: {}\\nIs the possible answer: (A) True (B) False\\nThe possible answer is:\"\n",
        "\n",
        "    inputs = qa_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    # print(qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = qa_model(**inputs)\n",
        "        start_logits = outputs.start_logits[0]\n",
        "        end_logits = outputs.end_logits[0]\n",
        "\n",
        "        # Find the token index of \"True\" in the prompt\n",
        "        true_token_id = qa_tokenizer.convert_tokens_to_ids(\"ĠTrue\")\n",
        "        true_token_indices = (inputs[\"input_ids\"][0] == true_token_id).nonzero(as_tuple=True)[0]\n",
        "        if true_token_indices.nelement() == 0:\n",
        "          print(f\"Warning: 'True' token not found in prompt for question: {question}\")\n",
        "          return 0.5\n",
        "        else:\n",
        "          true_token_index = true_token_indices[0].item()\n",
        "        # Calculate the score for each answer span based on start/end logits\n",
        "        answer_span_scores = []\n",
        "        for start_idx, start_logit in enumerate(start_logits):\n",
        "            for end_idx, end_logit in enumerate(end_logits[start_idx:]):\n",
        "                real_end_idx = start_idx + end_idx\n",
        "                if start_idx <= true_token_index <= real_end_idx:  # Check if span includes \"True\"\n",
        "                    score = start_logit + end_logit\n",
        "                    answer_span_scores.append(score)\n",
        "                else:\n",
        "                    answer_span_scores.append(torch.tensor(-float(\"inf\")).to(device))  # Assign very low score\n",
        "\n",
        "        # Find the answer span with the highest score\n",
        "        scores_tensor = torch.tensor(answer_span_scores)\n",
        "        scores_softmax = F.softmax(scores_tensor, dim=0)\n",
        "        p_true = scores_softmax[torch.argmax(scores_tensor)].item()\n",
        "\n",
        "    return p_true\n",
        "def calculate_lexical_similarity(answers, reference_answers):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "    scores = []\n",
        "    for answer in answers:\n",
        "        max_score = max(scorer.score(answer, ref)['rouge1'].fmeasure for ref in reference_answers)\n",
        "        scores.append(max_score)\n",
        "    average_score = sum(scores) / len(scores)\n",
        "    return average_score\n",
        "\n",
        "def calculate_semantic_entropy(cluster_probabilities):\n",
        "    entropy = 0\n",
        "    for prob in cluster_probabilities:\n",
        "        if prob > 0:\n",
        "            entropy -= prob * math.log2(prob)\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "-pMW8XG1yW40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_seq_log_prob(question, context, answer, model, tokenizer):\n",
        "    input_text = question + ' ' + context\n",
        "    output_text = answer\n",
        "\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    output_ids = tokenizer.encode(output_text, return_tensors='pt').to(device)\n",
        "\n",
        "    log_prob_sum = 0\n",
        "\n",
        "    for i in range(1, len(output_ids[0])):\n",
        "        previous_tokens = output_ids[:, :i]\n",
        "        target_token = output_ids[:, i]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(previous_tokens)\n",
        "            logits = outputs.logits[:, i-1, :]\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            log_prob = log_probs[0, target_token].item()\n",
        "\n",
        "        log_prob_sum += log_prob\n",
        "\n",
        "    seq_log_prob = log_prob_sum / len(output_ids[0])\n",
        "    return seq_log_prob"
      ],
      "metadata": {
        "id": "xlUO0eKtm-05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_question_data = []\n",
        "correct_output = []\n",
        "max_iterations = 1\n",
        "all_context_data=[]\n",
        "x=0"
      ],
      "metadata": {
        "id": "tKvGTYkMyavA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for context_idx, squad_example in enumerate(squad_examples):\n",
        "    print(\"iteration number: \",x)\n",
        "    if x>= max_iterations:\n",
        "        break\n",
        "    x+=1\n",
        "    context_data = {\"context_index\": context_idx, \"questions\": []}\n",
        "    print(\"PARA ITERATION\")\n",
        "    k = 0\n",
        "    for paragraph in squad_example['paragraphs']:\n",
        "        k += 1\n",
        "        if k % 10 == 0:\n",
        "            print(k)\n",
        "        context = paragraph['context']\n",
        "        for qa in paragraph['qas']:\n",
        "            question = qa['question']\n",
        "            reference_answers = [answer['text'] for answer in qa['answers']]\n",
        "            question_data = {\"question\": question, \"reference_answers\": reference_answers, \"generated_answers\": []}\n",
        "\n",
        "            # Generate answers using the model (on GPU)\n",
        "            model_answers_with_confidence = generate_answers(question, context, top_n=5)\n",
        "\n",
        "            # Evaluate and store correct/incorrect labels\n",
        "            for answer, confidence in model_answers_with_confidence:\n",
        "                if not reference_answers:\n",
        "                    print(f\"Skipping question with no reference answers: {question}\")\n",
        "                    continue\n",
        "                rouge_score = rouge_metric.compute(predictions=[answer], references=reference_answers, rouge_types=[\"rougeL\"])\n",
        "                rouge_l_score = rouge_score[\"rougeL\"].mid.fmeasure\n",
        "                correct_output.append(1 if rouge_l_score > 0.3 else 0)\n",
        "                question_data[\"generated_answers\"].append({\"answer\": answer, \"confidence\": confidence, \"rougeL_score\": rouge_l_score, \"correct\": correct_output[-1]})\n",
        "\n",
        "            # Calculate p(True) and store it in question_data\n",
        "            question_data[\"p_true\"] = calculate_ptrue(question, model_answers_with_confidence, top_n=5)\n",
        "\n",
        "            # Calculate lexical similarity and store it in question_data\n",
        "            question_data[\"lexical_similarity\"] = calculate_lexical_similarity([answer[0] for answer in model_answers_with_confidence], reference_answers)\n",
        "\n",
        "            # Cluster answers for the current question\n",
        "            clusters = cluster_answers(model_answers_with_confidence, question)\n",
        "            question_data[\"clusters\"] = clusters\n",
        "\n",
        "            context_data[\"questions\"].append(question_data)\n",
        "\n",
        "    all_context_data.append(context_data)"
      ],
      "metadata": {
        "id": "eXJYXsNSydko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42a8dd9-e684-459e-ee6a-da0169a9052b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration number:  0\n",
            "PARA ITERATION\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "iteration number:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average clusters for all questions\n",
        "average_correct_clusters, average_incorrect_clusters = calculate_average_clusters([q for c in all_context_data for q in c[\"questions\"]])\n",
        "\n",
        "# Add average cluster information to each context\n",
        "for context_data in all_context_data:\n",
        "    context_data[\"average_correct_clusters\"] = average_correct_clusters\n",
        "    context_data[\"average_incorrect_clusters\"] = average_incorrect_clusters\n",
        "\n",
        "# Store data in a JSON file\n",
        "with open(\"context_question_clustering_data.json\", \"w\") as f:\n",
        "    json.dump(all_context_data, f, indent=4)"
      ],
      "metadata": {
        "id": "jzzWm8tuylbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the data and calculate sequence log probabilities\n",
        "with open(\"context_question_clustering_data.json\", \"r\") as f:\n",
        "    all_context_data = json.load(f)\n",
        "for context_data in all_context_data:\n",
        "    context_index = context_data[\"context_index\"]\n",
        "    context = squad_examples[context_index][\"paragraphs\"][0][\"context\"]\n",
        "    for question_data in context_data[\"questions\"]:\n",
        "        question = question_data[\"question\"]\n",
        "        reference_answers = question_data[\"reference_answers\"]\n",
        "        generated_answers = question_data[\"generated_answers\"]\n",
        "        log_probs = []\n",
        "        for answer_dict in generated_answers:\n",
        "            answer = answer_dict[\"answer\"]\n",
        "            log_prob = calculate_seq_log_prob(question, context, answer, clm_model, clm_tokenizer)\n",
        "            log_probs.append(log_prob)\n",
        "            answer_dict[\"log_prob\"] = log_prob\n",
        "        avg_log_prob = sum(log_probs) / len(log_probs)\n",
        "        question_data[\"avg_log_prob\"] = avg_log_prob\n",
        "with open(\"context_question_clustering_data.json\", \"w\") as f:\n",
        "    json.dump(all_context_data, f, indent=4)"
      ],
      "metadata": {
        "id": "Zca6K22lynhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_question_entropy_data=[]\n",
        "# Calculate semantic entropy for each question and store in a new JSON fileall_question_entropy_data = []\n",
        "with open(\"context_question_clustering_data.json\", \"r\") as f:\n",
        "    all_context_data = json.load(f)\n",
        "for context_data in all_context_data:\n",
        "    for question_data in context_data[\"questions\"]:\n",
        "        clusters = question_data[\"clusters\"]\n",
        "        answer_confidences = {answer[\"answer\"]: answer[\"confidence\"] for answer in question_data[\"generated_answers\"]}\n",
        "        cluster_probabilities = [\n",
        "            sum(answer_confidences[answer] for answer in cluster) for cluster in clusters\n",
        "        ]\n",
        "        semantic_entropy = calculate_semantic_entropy(cluster_probabilities)\n",
        "        question_data[\"semantic_entropy\"] = semantic_entropy\n",
        "        all_question_entropy_data.append(question_data)\n",
        "with open(\"question_entropy_data.json\", \"w\") as f:\n",
        "    json.dump(all_question_entropy_data, f, indent=4)"
      ],
      "metadata": {
        "id": "uoSZPNNhnYrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ptrue_values = []\n",
        "lexical_similarity_values = []\n",
        "entropy_values = []\n",
        "correct_labels = []\n",
        "for question_data in all_question_entropy_data:\n",
        "    for answer in question_data[\"generated_answers\"]:\n",
        "        ptrue_values.append(question_data[\"p_true\"])\n",
        "        lexical_similarity_values.append(question_data[\"lexical_similarity\"])\n",
        "        entropy_values.append(question_data[\"semantic_entropy\"])\n",
        "        correct_labels.append(answer[\"correct\"])\n",
        "\n",
        "# Calculate AUROC scores\n",
        "auroc_ptrue = roc_auc_score(correct_labels, ptrue_values)\n",
        "auroc_lexical = roc_auc_score(correct_labels, lexical_similarity_values)\n",
        "auroc_entropy = roc_auc_score(correct_labels, entropy_values)\n",
        "\n",
        "# Store AUROC scores in a dictionary\n",
        "auroc_scores = {\n",
        "    \"p_true\": auroc_ptrue,\n",
        "    \"lexical_similarity\": auroc_lexical,\n",
        "    \"semantic_entropy\": auroc_entropy\n",
        "}\n",
        "\n",
        "# Save AUROC scores to a JSON file\n",
        "with open(\"auroc_scores.json\", \"w\") as f:\n",
        "    json.dump(auroc_scores, f, indent=4)\n",
        "\n",
        "# Print AUROC scores\n",
        "print(\"AUROC Scores:\")\n",
        "for metric, score in auroc_scores.items():\n",
        "    print(f\"{metric}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "O908qA_Tyr1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "auroc_scores_path = \"auroc_scores.json\"\n",
        "with open(auroc_scores_path, \"r\") as f:\n",
        "    auroc_scores = json.load(f)\n",
        "\n",
        "with open(\"context_question_clustering_data.json\", \"r\") as f:\n",
        "    context_questions_data = json.load(f)\n",
        "\n",
        "total_clusters = sum(len(question['clusters']) for context in context_questions_data for question in context['questions'])\n",
        "total_answers = sum(len(question['generated_answers']) for context in context_questions_data for question in context['questions'])\n",
        "distinct_percentage = (total_clusters/total_answers)\n",
        "\n",
        "\n",
        "# Create a histogram of the AUROC scores\n",
        "fig, ax = plt.subplots()\n",
        "auroc_values = list(auroc_scores.values())\n",
        "labels = list(auroc_scores.keys())\n",
        "ax.bar(labels, auroc_values, color=['blue', 'orange', 'green'])\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_ylabel('AUROC')\n",
        "ax.set_title('AUROC Scores for Different Uncertainty Calculations')\n",
        "plt.ylim([0.0, 1.0])  # Assuming AUROC scores are between 0.5 and 1.0\n",
        "plt.show()\n",
        "\n",
        "# Now, let's create the table. For this example, we will simulate some data.\n",
        "# The actual implementation should use the results from the actual data processing.\n",
        "\n",
        "# Simulating some data for the table, in practice you should extract this from your results\n",
        "# num_correct = sum(1 for output in correct if output == 1)\n",
        "# percentage_correct = (num_correct / len(correct)) * 100\n",
        "table_data = {\n",
        "    \"Metric\": [\"Semantic Entropy\", \"Number of Distinct Answers\"],\n",
        "    # \"% Correctly Answered\":percentage_correct,\n",
        "    \"AUROC Score\": [auroc_scores.get(\"semantic_entropy\", 0), None],  # None for placeholders\n",
        "    \"% of Distinct Answers\": [None,distinct_percentage]  # None for placeholders\n",
        "}\n",
        "\n",
        "# Convert the data to a pandas DataFrame\n",
        "df_table = pd.DataFrame(table_data)\n",
        "\n",
        "# Display the table\n",
        "print(df_table)\n",
        "\n",
        "# Note: The histogram has been created with matplotlib as requested, and the table has been displayed.\n",
        "# For a real application, the table data should not be simulated but extracted from your processed data.\n"
      ],
      "metadata": {
        "id": "mfhQh8N2ZoZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the question entropy data\n",
        "with open('question_entropy_data.json', 'r') as file:\n",
        "    entropy_data = json.load(file)\n",
        "    generated_data = json.load(file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Load the generated answers data\n",
        "questions = []\n",
        "average_rouge_scores = []\n",
        "average_confidence = []\n",
        "p_true_values = []\n",
        "lexical_similarities = []\n",
        "semantic_entropies = []\n",
        "average_log_probs = []\n",
        "\n",
        "# Iterate over each question in the generated data\n",
        "for context in entropy_data:\n",
        "    # Iterate over each question in the context\n",
        "    for question_data in context['questions']:\n",
        "        question_text = question_data['question']\n",
        "        questions.append(question_text)\n",
        "\n",
        "        # Extract relevant data\n",
        "        generated_answers = question_data['generated_answers']\n",
        "        avg_log_prob = [gen_ans['avg_log_prob'] for gen_ans in generated_answers]\n",
        "        average_log_probs.append(avg_log_prob)\n",
        "\n",
        "# Iterate over each entry in the entropy data\n",
        "for entry in entropy_data:\n",
        "    question_text = entry['question']\n",
        "    avg_log_prob = entry['avg_log_prob']\n",
        "    rouge_scores = [gen_ans['rougeL_score'] for gen_ans in entry['generated_answers']]\n",
        "    confidences = [gen_ans['confidence'] for gen_ans in entry['generated_answers']]\n",
        "\n",
        "    # # Calculate average ROUGE-L score and confidence\n",
        "    # avg_rouge_score = np.mean(rouge_scores)\n",
        "    # avg_confidence = np.mean(confidences)\n",
        "\n",
        "    # # Append calculated values to lists\n",
        "    average_rouge_scores.extend(rouge_scores)\n",
        "    average_confidence.extend(confidences)\n",
        "    p_true_values.append(entry['p_true'])\n",
        "    lexical_similarities.append(entry['lexical_similarity'])\n",
        "    semantic_entropies.append(entry['semantic_entropy'])\n",
        "    average_log_probs.extend(avg_log_prob)\n",
        "\n",
        "# Calculate overall averages\n",
        "avg_rouge_score = np.mean(average_rouge_scores)\n",
        "avg_confidence = np.mean(average_confidence)\n",
        "avg_p_true = np.mean(p_true_values)\n",
        "avg_lexical_similarity = np.mean(lexical_similarities)\n",
        "avg_semantic_entropy = np.mean(semantic_entropies)\n",
        "avg_average_log_prob = np.mean(average_log_probs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plotting the overall average metrics\n",
        "metrics = ['Average ROUGE-L Score', 'Average Confidence', 'P_True Values',\n",
        "           'Lexical Similarity', 'Semantic Entropy', 'Average Log Probability']\n",
        "averages = [avg_rouge_score, avg_confidence, avg_p_true,\n",
        "            avg_lexical_similarity, avg_semantic_entropy, avg_average_log_prob]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "ax.bar(metrics, averages, alpha=0.7)\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_ylabel('Average Values')\n",
        "ax.set_title('Overall Average Metrics for All Questions')\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "ax.grid(axis='y')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# # Plotting only for the first 10 questions\n",
        "# x = np.arange(10)\n",
        "# width = 0.15\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# rects1 = ax.bar(x - 2*width, average_rouge_scores[:10], width, label='Average ROUGE-L Score', alpha=0.7)\n",
        "# rects2 = ax.bar(x - width, average_confidence[:10], width, label='Average Confidence', alpha=0.7)\n",
        "# rects3 = ax.bar(x, p_true_values[:10], width, label='P_True Values', alpha=0.7)\n",
        "# rects4 = ax.bar(x + width, lexical_similarities[:10], width, label='Lexical Similarity', alpha=0.7)\n",
        "# rects5 = ax.bar(x + 2*width, semantic_entropies[:10], width, label='Semantic Entropy', alpha=0.7)\n",
        "# rects6 = ax.bar(x + 3*width, [np.mean(log_probs) for log_probs in average_log_probs[:10]], width, label='Average Log Probability', alpha=0.7)\n",
        "\n",
        "# ax.set_xlabel('Questions')\n",
        "# ax.set_ylabel('Values')\n",
        "# ax.set_title('Average Metrics for First 10 Questions')\n",
        "# ax.set_xticks(x)\n",
        "# ax.set_xticklabels(questions[:10], rotation=90)\n",
        "# ax.legend()\n",
        "\n",
        "# fig.tight_layout()\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "0nft4z-wC8x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-rFcTRTnFkiw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}